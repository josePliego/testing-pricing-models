%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Jose Pliego at 2023-01-30 19:51:47 -0500


%% Saved with string encoding Unicode (UTF-8)



@misc{allstate-claims-severity,
    author = {{Allstate Insurance}},
    title = {Allstate Claims Severity},
    publisher = {Kaggle},
    year = {2016},
    url = {https://kaggle.com/competitions/allstate-claims-severity}
}

@book{esl,
	address = {New York, NY, USA},
	author = {Trevor Hastie and Robert Tibshirani and Jerome Friedman},
	date-modified = {2022-08-18 13:17:23 -0400},
	edition = {2},
	publisher = {Springer New York Inc.},
	series = {Springer Series in Statistics},
	title = {The Elements of Statistical Learning},
	year = 2017}

@inproceedings{bouckaert,
	abstract = {Empirical research in learning algorithms for classification tasks generally requires the use of significance tests. The quality of a test is typically judged on Type I error (how often the test indicates a difference when it should not) and Type II error (how often it indicates no difference when it should). In this paper we argue that the replicability of a test is also of importance. We say that a test has low replicability if its outcome strongly depends on the particular random partitioning of the data that is used to perform it. We present empirical measures of replicability and use them to compare the performance of several popular tests in a realistic setting involving standard learning algorithms and benchmark datasets. Based on our results we give recommendations on which test to use.},
	address = {Berlin, Heidelberg},
	author = {Bouckaert, Remco R. and Frank, Eibe},
	booktitle = {Advances in Knowledge Discovery and Data Mining},
	editor = {Dai, Honghua and Srikant, Ramakrishnan and Zhang, Chengqi},
	isbn = {978-3-540-24775-3},
	pages = {3--12},
	publisher = {Springer Berlin Heidelberg},
	title = {Evaluating the Replicability of Significance Tests for Comparing Learning Algorithms},
	year = {2004}}

@article{ttest,
	abstract = {In order to compare learning algorithms, experimental results reported in the machine learning literature often use statistical tests of significance to support the claim that a new learning algorithm generalizes better. Such tests should take into account the variability due to the choice of training set and not only that due to the test examples, as is often the case. This could lead to gross underestimation of the variance of the cross-validation estimator, and to the wrong conclusion that the new algorithm is significantly better when it is not. We perform a theoretical investigation of the variance of a variant of the cross-validation estimator of the generalization error that takes into account the variability due to the randomness of the training set as well as test examples. Our analysis shows that all the variance estimators that are based only on the results of the cross-validation experiment must be biased. This analysis allows us to propose new estimators of this variance. We show, via simulations, that tests of hypothesis about the generalization error using those new variance estimators have better properties than tests involving variance estimators currently in use and listed in Dietterich (1998). In particular, the new tests have correct size and good power. That is, the new tests do not reject the null hypothesis too often when the hypothesis is true, but they tend to frequently reject the null hypothesis when the latter is false.},
	author = {Nadeau, Claude and Bengio, Yoshua},
	date = {2003/09/01},
	date-added = {2022-08-18 13:20:33 -0400},
	date-modified = {2022-08-18 13:20:33 -0400},
	doi = {10.1023/A:1024068626366},
	id = {Nadeau2003},
	isbn = {1573-0565},
	journal = {Machine Learning},
	number = {3},
	pages = {239--281},
	title = {Inference for the Generalization Error},
	url = {https://doi.org/10.1023/A:1024068626366},
	volume = {52},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1023/A:1024068626366}}

@misc{optuna,
  doi = {10.48550/ARXIV.1907.10902},
  
  url = {https://arxiv.org/abs/1907.10902},
  
  author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Optuna: A Next-generation Hyperparameter Optimization Framework},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{lightgbm,
    author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
        title = {LightGBM: A Highly Efficient Gradient Boosting Decision Tree},
        year = {2017},
        isbn = {9781510860964},
        publisher = {Curran Associates Inc.},
        address = {Red Hook, NY, USA},
        abstract = {Gradient Boosting Decision Tree (GBDT) is a popular machine learning algorithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efficiency and scalability are still unsatisfactory when the feature dimension is high and data size is large. A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming. To tackle this problem, we propose two novel techniques: Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB). With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. We prove that finding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio (and thus can effectively reduce the number of features without hurting the accuracy of split point determination by much). We call our new GBDT implementation with GOSS and EFB LightGBM. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy.},
        booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
        pages = {3149â€“3157},
        numpages = {9},
        location = {Long Beach, California, USA}, series = {NIPS'17}
}

@inproceedings{xgboost,
    doi = {10.1145/2939672.2939785},
  
    url = {https://doi.org/10.1145%2F2939672.2939785},
  
    year = 2016,
    month = {aug},
  
    publisher = {{ACM}
},
  
    author = {Tianqi Chen and Carlos Guestrin},
  
    title = {{XGBoost}},
  
    booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining}
}
